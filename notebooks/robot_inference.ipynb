{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import PIL\n",
    "import google.generativeai as genai\n",
    "import rerun as rr\n",
    "from faster_whisper import WhisperModel\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.policies.act.modeling_act import ACTPolicy\n",
    "from lerobot.common.policies.pi0.modeling_pi0 import PI0Policy\n",
    "from lerobot.common.policies.pi0fast.modeling_pi0fast import PI0FASTPolicy\n",
    "from lerobot.common.robot_devices.cameras.configs import OpenCVCameraConfig\n",
    "from lerobot.common.robot_devices.robots.utils import make_robot_from_config\n",
    "from lerobot.common.robot_devices.control_utils import busy_wait, log_control_info\n",
    "from lerobot.common.robot_devices.robots.configs import So100RobotConfig\n",
    "from lerobot.common.vision_utils.gemini_perception import tensor_to_pil, get_2D_bbox, parse_json, normalize_bbox_0to1, plot_bbox, get_target_bbox, get_random_targets, create_pick_place_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_key_listener():\n",
    "    events = {\"exit_early\": False, \"rerecord_episode\": False, \n",
    "              \"stop_recording\": False, \"select_new_bbox\": False}\n",
    "    \n",
    "    from pynput import keyboard\n",
    "    \n",
    "    def on_press(key):\n",
    "        try:\n",
    "            if key == keyboard.Key.right:\n",
    "                print(\"Right arrow key pressed. Exiting loop...\")\n",
    "                events[\"exit_early\"] = True\n",
    "            elif key == keyboard.Key.space:\n",
    "                print(\"Space key pressed. Selecting new bbox...\")\n",
    "                events[\"select_new_bbox\"] = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error handling key press: {e}\")\n",
    "    \n",
    "    listener = keyboard.Listener(on_press=on_press)\n",
    "    listener.start()\n",
    "    \n",
    "    return listener, events\n",
    "\n",
    "def return_bbox(boxes, idx):\n",
    "    pick_t = boxes[idx]['box_2d']\n",
    "    norm_pick_t = [p/1000 for p in pick_t]\n",
    "    return norm_pick_t, pick_t\n",
    "\n",
    "def select_bbox(boxes):\n",
    "    selection = int(input(\"Enter the number of the bounding box you want to select: \"))\n",
    "\n",
    "    # Validate selection\n",
    "    if 0 <= selection < len(boxes):return return_bbox(boxes,selection)\n",
    "    else:print(f\"Invalid selection. Please choose a number between 1 and {len(boxes)}\")\n",
    "\n",
    "def iterate_over_bbox(boxes):\n",
    "    for i in range(len(boxes)):\n",
    "        norm_pick_t, pick_t = return_bbox(boxes,i)\n",
    "        yield norm_pick_t, pick_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting main follower arm.\n",
      "Connecting main leader arm.\n",
      "Activating torque on main follower arm.\n"
     ]
    }
   ],
   "source": [
    "# Create camera config using proper config objects\n",
    "cameras = {\n",
    "    \"laptop\": OpenCVCameraConfig(\n",
    "        camera_index=0,  # Built-in webcam\n",
    "        fps=30,\n",
    "        width=640,\n",
    "        height=480\n",
    "    ),\n",
    "#    \"top\": OpenCVCameraConfig(\n",
    "#        camera_index=1,  # iPhone camera\n",
    "#        fps=30,\n",
    "#        width=640,\n",
    "#        height=480\n",
    "#    )\n",
    "}\n",
    "\n",
    "robot_cfg = So100RobotConfig(\n",
    "            cameras=cameras,\n",
    "            mock=False, \n",
    "            \n",
    "        )\n",
    "        \n",
    "        # Create and connect robot\n",
    "robot = make_robot_from_config(robot_cfg)\n",
    "robot.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_time_s = 100\n",
    "fps = 30\n",
    "device = \"mps\"  # TODO: On Mac, use \"mps\" or \"cpu\"\n",
    "BOX_AUGMENTATION = True\n",
    "\n",
    "# audio configs\n",
    "SAMPLE_RATE = 16_000   # Whisper works best on 16 kHz mono\n",
    "DURATION = 7           # seconds (max length requested)\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"/Users/shreyas/Downloads/robot/audio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Whisper configs\n",
    "model_size = \"medium\"  # options: tiny, base, small, medium, large-v3\n",
    "model = WhisperModel(model_size, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_path = \"/Users/shreyas/Downloads/act_bbox/100000/pretrained_model\"\n",
    "pi0_path = \"/Users/shreyas/Downloads/pi0/100000/pretrained_model\"\n",
    "pi0fast_path = \"/Users/shreyas/Downloads/pi0fast/040000/pretrained_model\"\n",
    "\n",
    "#policy = ACTPolicy.from_pretrained(act_path)\n",
    "policy = PI0Policy.from_pretrained(pi0_path)\n",
    "#policy = PI0FASTPolicy.from_pretrained(pi0fast_path)\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ™ï¸ Recording for\", DURATION, \"secondsâ€¦\")\n",
    "audio = sd.rec(int(DURATION * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype='int16')\n",
    "sd.wait()  # wait until recording is finished\n",
    "    \n",
    "# Convert numpy array to MP3 using pydub\n",
    "audio_segment = AudioSegment(\n",
    "    audio.tobytes(), \n",
    "    frame_rate=SAMPLE_RATE,\n",
    "    sample_width=audio.dtype.itemsize, \n",
    "    channels=1\n",
    ")\n",
    "    \n",
    "output_file = os.path.join(output_dir, \"command1.mp3\")\n",
    "audio_segment.export(output_file, format=\"mp3\")\n",
    "print(\" Saved audio to\", output_file)\n",
    "    \n",
    "segments, info = model.transcribe(\"/Users/shreyas/Downloads/robot/audio/command1.mp3\")\n",
    "print(f\"Detected language: {info.language} â€” probability {info.language_probability:.2%}\\n\")\n",
    "\n",
    "user_task = \" \".join(s.text.strip() for s in segments)\n",
    "print(\"ðŸ“ Transcript:\", user_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_task = \"Add all wooden blocks to the blue bin\"\n",
    "prompt = \"\"\"User request: \"\"\" + user_task + \"\"\"\n",
    "\n",
    "    Analyze the provided image and understand what the user needs. Identify all relevant objects on the desk that would help fulfill this request.\n",
    "    \n",
    "    For example:\n",
    "    - If the user wants to build something (like a red lego wall or wooden tower or lego plane), find all appropriate building pieces (Focus on identifying all red lego bricks on the desk that could be used for building a wall. Or in case of wooden tower, identify all wooden blocks on the desk that could be used for building a tower or all lego pieces on the desk that could be used to build a plane.)\n",
    "    - If the user wants to clear the desk, identify all objects on the desk that need to be cleared\n",
    "    - If the user wants specific colored items, focus on those colors\n",
    "    - If the users mentions place location, identify the location of the object on the desk\n",
    "    - If user is pointing to an object, identify the object that the user is pointing to.\n",
    "    \n",
    "    Ignore the robot arm itself if visible.\n",
    "    Return your findings strictly as a JSON array of bounding boxes.\n",
    "    Example format: [{\"label\": \"red lego brick\", \"box_2d\": [100, 200, 150, 280]}, {\"label\": \"blue bin\", \"box_2d\": [500, 600, 700, 850]}]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = robot.capture_observation()\n",
    "pick, place, norm_pick, norm_place, pick_labels, place_labels = get_target_bbox(observation[\"observation.images.laptop\"], prompt=prompt)\n",
    "print(\"Pick objects:\")\n",
    "for i in range(len(pick)):print(pick_labels[i])\n",
    "print(\"Place location:\")\n",
    "for i in range(len(place)):print(place_labels[i], place[i])\n",
    "pick_t, place_t, norm_pick_t, norm_place_t, pick, norm_pick, pick_target_label, place_target_label, pick_labels = get_random_targets(pick, place, norm_pick, norm_place, pick_labels, place_labels)\n",
    "single_task = f\"Grasp {pick_target_label} and put it in {place_target_label}\"\n",
    "listener, events = setup_key_listener()\n",
    "\n",
    "for _ in range(inference_time_s * fps):\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Read the follower state and access the frames from the cameras\n",
    "    observation = robot.capture_observation()\n",
    "    observation[\"task\"] = single_task\n",
    "    observation[\"observation.state\"] = torch.cat([observation[\"observation.state\"], norm_pick_t])\n",
    "    # Convert to pytorch format: channel first and float32 in [0,1]\n",
    "    # with batch dimension\n",
    "    for name in observation:\n",
    "        if name == \"task\":\n",
    "            # Skip tensor operations for string values\n",
    "            observation[name] = [observation[name]]  # Wrap in list to simulate batch dimension\n",
    "            continue\n",
    "        if \"image\" in name:\n",
    "            observation[name] = observation[name].type(torch.float32) / 255\n",
    "            observation[name] = observation[name].permute(2, 0, 1).contiguous()\n",
    "        observation[name] = observation[name].unsqueeze(0)\n",
    "        observation[name] = observation[name].to(device)\n",
    "\n",
    "    # Compute the next action with the policy\n",
    "        # based on the current observation\n",
    "    action = policy.select_action(observation)\n",
    "    # Remove batch dimension\n",
    "    action = action.squeeze(0)\n",
    "    # Move to cpu, if not already the case\n",
    "    action = action.to(\"cpu\")\n",
    "    # Order the robot to move\n",
    "    #print(action[-2])\n",
    "    robot.send_action(action)\n",
    "        \n",
    "    dt_s = time.perf_counter() - start_time\n",
    "    busy_wait(1 / fps - dt_s)\n",
    "        \n",
    "    if events[\"select_new_bbox\"]:\n",
    "        events[\"select_new_bbox\"] = False\n",
    "        pick_t, place_t, norm_pick_t, norm_place_t, pick, norm_pick, pick_target_label, place_target_label, pick_labels = get_random_targets(pick, place, norm_pick, norm_place, pick_labels, place_labels)\n",
    "        single_task = f\"Grasp {pick_target_label} and put it in {place_target_label}\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caferacer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
